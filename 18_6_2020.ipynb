{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gS29lsQFoHE"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "He-gGAj-Ftsc"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zRmagsR1F0vx",
    "outputId": "5ffd4378-c961-45f6-8bdb-ad7fa0cad7bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "Hit Enter to continue: \n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "Hit Enter to continue: \n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> \n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2z8gsWAF2Hr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2ULVliJHX8n"
   },
   "outputs": [],
   "source": [
    "# Find the element on the webpage\n",
    "\n",
    "Text = \"\"\"Simplilearn is ranked as #8 of the 50 most influential education brands outpacing Wharton University, Harvard Business School, New York University, University of Phoenix and other traditional institutions. Organizations were honored for their achievements in the areas of educational publishing and their commitment to providing quality content. As one of the top among 50 organizations receiving recognition, Simplilearn was selected as #8 based on its engaging work on LinkedIn.\n",
    "\n",
    "Michael Stebbins, Simplilearn's Chief Innovation Officer, said “This is another validation that skill-centric education is the new way to improve careers. We're among good company with LinkedIn and others who join us in engaging professional audiences with information that not only educates but also inspires them to reach their career goals.”\n",
    "\n",
    "Education sector marketers have flocked to LinkedIn to reach the professional networking platform’s audience of professionals, who are keen to boost their careers and earning power with further education, according to the report.\n",
    "\n",
    "Simplilearn is the world’s largest provider of short-term certification courses that address unique learning needs of working professionals. Courses include a variety of topics in the business, technology, and design fields, spanning PMP, Big Data, Digital Marketing, Data Science, Cloud Computing and many more. With over 400 courses offered, Simplilearn has trained more than 500,000 professionals since its inception in April 2010. With its footprint in over 150 countries, Simplilearn’s patrons are assured of up-skilling and re-skilling themselves for faster and surer career growth. Simplilearn’s course content is recognized by more than 40 global accreditors, including PMI, Axelos, Scrum Alliance, OMCP, Peoplecert, and The Open Group. For more information, visit http://www.simplilearn.com \n",
    "(Attn.editors: The above press release comes to you under an arrangement with Business Wire India. THE HINDU takes no Editorial responsibility for the same.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZEW3odle4G55",
    "outputId": "f9b88e06-da8e-479e-9682-4db057b21c06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x9gnp8e6HbtJ",
    "outputId": "b968b3e6-0887-4f5b-9a94-44ccbfb99d61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Z6IVhD4XHeMc",
    "outputId": "9d98945b-4e4b-4893-cc02-32cbd6b8cf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "SL_word_tokenize = word_tokenize(Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bkRmEBafHiw1",
    "outputId": "3f80df96-f3f9-4205-d374-e4f12c1c1afc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Simplilearn',\n",
       " 'is',\n",
       " 'ranked',\n",
       " 'as',\n",
       " '#',\n",
       " '8',\n",
       " 'of',\n",
       " 'the',\n",
       " '50',\n",
       " 'most',\n",
       " 'influential',\n",
       " 'education',\n",
       " 'brands',\n",
       " 'outpacing',\n",
       " 'Wharton',\n",
       " 'University',\n",
       " ',',\n",
       " 'Harvard',\n",
       " 'Business',\n",
       " 'School',\n",
       " ',',\n",
       " 'New',\n",
       " 'York',\n",
       " 'University',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Phoenix',\n",
       " 'and',\n",
       " 'other',\n",
       " 'traditional',\n",
       " 'institutions',\n",
       " '.',\n",
       " 'Organizations',\n",
       " 'were',\n",
       " 'honored',\n",
       " 'for',\n",
       " 'their',\n",
       " 'achievements',\n",
       " 'in',\n",
       " 'the',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'educational',\n",
       " 'publishing',\n",
       " 'and',\n",
       " 'their',\n",
       " 'commitment',\n",
       " 'to',\n",
       " 'providing',\n",
       " 'quality',\n",
       " 'content',\n",
       " '.',\n",
       " 'As',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'top',\n",
       " 'among',\n",
       " '50',\n",
       " 'organizations',\n",
       " 'receiving',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'Simplilearn',\n",
       " 'was',\n",
       " 'selected',\n",
       " 'as',\n",
       " '#',\n",
       " '8',\n",
       " 'based',\n",
       " 'on',\n",
       " 'its',\n",
       " 'engaging',\n",
       " 'work',\n",
       " 'on',\n",
       " 'LinkedIn',\n",
       " '.',\n",
       " 'Michael',\n",
       " 'Stebbins',\n",
       " ',',\n",
       " 'Simplilearn',\n",
       " \"'s\",\n",
       " 'Chief',\n",
       " 'Innovation',\n",
       " 'Officer',\n",
       " ',',\n",
       " 'said',\n",
       " '“',\n",
       " 'This',\n",
       " 'is',\n",
       " 'another',\n",
       " 'validation',\n",
       " 'that',\n",
       " 'skill-centric',\n",
       " 'education',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'way',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'careers',\n",
       " '.',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'among',\n",
       " 'good',\n",
       " 'company',\n",
       " 'with',\n",
       " 'LinkedIn',\n",
       " 'and',\n",
       " 'others',\n",
       " 'who',\n",
       " 'join',\n",
       " 'us',\n",
       " 'in',\n",
       " 'engaging',\n",
       " 'professional',\n",
       " 'audiences',\n",
       " 'with',\n",
       " 'information',\n",
       " 'that',\n",
       " 'not',\n",
       " 'only',\n",
       " 'educates',\n",
       " 'but',\n",
       " 'also',\n",
       " 'inspires',\n",
       " 'them',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'their',\n",
       " 'career',\n",
       " 'goals.',\n",
       " '”',\n",
       " 'Education',\n",
       " 'sector',\n",
       " 'marketers',\n",
       " 'have',\n",
       " 'flocked',\n",
       " 'to',\n",
       " 'LinkedIn',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'the',\n",
       " 'professional',\n",
       " 'networking',\n",
       " 'platform',\n",
       " '’',\n",
       " 's',\n",
       " 'audience',\n",
       " 'of',\n",
       " 'professionals',\n",
       " ',',\n",
       " 'who',\n",
       " 'are',\n",
       " 'keen',\n",
       " 'to',\n",
       " 'boost',\n",
       " 'their',\n",
       " 'careers',\n",
       " 'and',\n",
       " 'earning',\n",
       " 'power',\n",
       " 'with',\n",
       " 'further',\n",
       " 'education',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'the',\n",
       " 'report',\n",
       " '.',\n",
       " 'Simplilearn',\n",
       " 'is',\n",
       " 'the',\n",
       " 'world',\n",
       " '’',\n",
       " 's',\n",
       " 'largest',\n",
       " 'provider',\n",
       " 'of',\n",
       " 'short-term',\n",
       " 'certification',\n",
       " 'courses',\n",
       " 'that',\n",
       " 'address',\n",
       " 'unique',\n",
       " 'learning',\n",
       " 'needs',\n",
       " 'of',\n",
       " 'working',\n",
       " 'professionals',\n",
       " '.',\n",
       " 'Courses',\n",
       " 'include',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'topics',\n",
       " 'in',\n",
       " 'the',\n",
       " 'business',\n",
       " ',',\n",
       " 'technology',\n",
       " ',',\n",
       " 'and',\n",
       " 'design',\n",
       " 'fields',\n",
       " ',',\n",
       " 'spanning',\n",
       " 'PMP',\n",
       " ',',\n",
       " 'Big',\n",
       " 'Data',\n",
       " ',',\n",
       " 'Digital',\n",
       " 'Marketing',\n",
       " ',',\n",
       " 'Data',\n",
       " 'Science',\n",
       " ',',\n",
       " 'Cloud',\n",
       " 'Computing',\n",
       " 'and',\n",
       " 'many',\n",
       " 'more',\n",
       " '.',\n",
       " 'With',\n",
       " 'over',\n",
       " '400',\n",
       " 'courses',\n",
       " 'offered',\n",
       " ',',\n",
       " 'Simplilearn',\n",
       " 'has',\n",
       " 'trained',\n",
       " 'more',\n",
       " 'than',\n",
       " '500,000',\n",
       " 'professionals',\n",
       " 'since',\n",
       " 'its',\n",
       " 'inception',\n",
       " 'in',\n",
       " 'April',\n",
       " '2010',\n",
       " '.',\n",
       " 'With',\n",
       " 'its',\n",
       " 'footprint',\n",
       " 'in',\n",
       " 'over',\n",
       " '150',\n",
       " 'countries',\n",
       " ',',\n",
       " 'Simplilearn',\n",
       " '’',\n",
       " 's',\n",
       " 'patrons',\n",
       " 'are',\n",
       " 'assured',\n",
       " 'of',\n",
       " 'up-skilling',\n",
       " 'and',\n",
       " 're-skilling',\n",
       " 'themselves',\n",
       " 'for',\n",
       " 'faster',\n",
       " 'and',\n",
       " 'surer',\n",
       " 'career',\n",
       " 'growth',\n",
       " '.',\n",
       " 'Simplilearn',\n",
       " '’',\n",
       " 's',\n",
       " 'course',\n",
       " 'content',\n",
       " 'is',\n",
       " 'recognized',\n",
       " 'by',\n",
       " 'more',\n",
       " 'than',\n",
       " '40',\n",
       " 'global',\n",
       " 'accreditors',\n",
       " ',',\n",
       " 'including',\n",
       " 'PMI',\n",
       " ',',\n",
       " 'Axelos',\n",
       " ',',\n",
       " 'Scrum',\n",
       " 'Alliance',\n",
       " ',',\n",
       " 'OMCP',\n",
       " ',',\n",
       " 'Peoplecert',\n",
       " ',',\n",
       " 'and',\n",
       " 'The',\n",
       " 'Open',\n",
       " 'Group',\n",
       " '.',\n",
       " 'For',\n",
       " 'more',\n",
       " 'information',\n",
       " ',',\n",
       " 'visit',\n",
       " 'http',\n",
       " ':',\n",
       " '//www.simplilearn.com',\n",
       " '(',\n",
       " 'Attn.editors',\n",
       " ':',\n",
       " 'The',\n",
       " 'above',\n",
       " 'press',\n",
       " 'release',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'you',\n",
       " 'under',\n",
       " 'an',\n",
       " 'arrangement',\n",
       " 'with',\n",
       " 'Business',\n",
       " 'Wire',\n",
       " 'India',\n",
       " '.',\n",
       " 'THE',\n",
       " 'HINDU',\n",
       " 'takes',\n",
       " 'no',\n",
       " 'Editorial',\n",
       " 'responsibility',\n",
       " 'for',\n",
       " 'the',\n",
       " 'same',\n",
       " '.',\n",
       " ')']"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3nxMlaVlHkcz",
    "outputId": "db883a3c-c368-478f-b4ec-0b0eb2aaeb69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SL_word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbNixXhkHoFV"
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vaIBEvvHqDo"
   },
   "outputs": [],
   "source": [
    "for word in SL_word_tokenize:\n",
    "    fdist[word.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "-3kakSqJLHR3",
    "outputId": "206239db-13bd-4efd-b630-725dc65930da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 24),\n",
       " ('the', 12),\n",
       " ('.', 12),\n",
       " ('of', 9),\n",
       " ('and', 9),\n",
       " ('to', 8),\n",
       " ('simplilearn', 7),\n",
       " ('with', 6),\n",
       " ('is', 5),\n",
       " ('in', 5),\n",
       " ('education', 4),\n",
       " ('for', 4),\n",
       " ('their', 4),\n",
       " ('’', 4),\n",
       " ('s', 4)]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "7KTloj5oLJnI",
    "outputId": "8ef4213c-34c7-4207-8330-94023c9c9278"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simplilearn</th>\n",
       "      <th>is</th>\n",
       "      <th>ranked</th>\n",
       "      <th>as</th>\n",
       "      <th>#</th>\n",
       "      <th>8</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>50</th>\n",
       "      <th>most</th>\n",
       "      <th>influential</th>\n",
       "      <th>education</th>\n",
       "      <th>brands</th>\n",
       "      <th>outpacing</th>\n",
       "      <th>wharton</th>\n",
       "      <th>university</th>\n",
       "      <th>,</th>\n",
       "      <th>harvard</th>\n",
       "      <th>business</th>\n",
       "      <th>school</th>\n",
       "      <th>new</th>\n",
       "      <th>york</th>\n",
       "      <th>phoenix</th>\n",
       "      <th>and</th>\n",
       "      <th>other</th>\n",
       "      <th>traditional</th>\n",
       "      <th>institutions</th>\n",
       "      <th>.</th>\n",
       "      <th>organizations</th>\n",
       "      <th>were</th>\n",
       "      <th>honored</th>\n",
       "      <th>for</th>\n",
       "      <th>their</th>\n",
       "      <th>achievements</th>\n",
       "      <th>in</th>\n",
       "      <th>areas</th>\n",
       "      <th>educational</th>\n",
       "      <th>publishing</th>\n",
       "      <th>commitment</th>\n",
       "      <th>to</th>\n",
       "      <th>...</th>\n",
       "      <th>surer</th>\n",
       "      <th>growth</th>\n",
       "      <th>course</th>\n",
       "      <th>recognized</th>\n",
       "      <th>by</th>\n",
       "      <th>40</th>\n",
       "      <th>global</th>\n",
       "      <th>accreditors</th>\n",
       "      <th>including</th>\n",
       "      <th>pmi</th>\n",
       "      <th>axelos</th>\n",
       "      <th>scrum</th>\n",
       "      <th>alliance</th>\n",
       "      <th>omcp</th>\n",
       "      <th>peoplecert</th>\n",
       "      <th>open</th>\n",
       "      <th>group</th>\n",
       "      <th>visit</th>\n",
       "      <th>http</th>\n",
       "      <th>:</th>\n",
       "      <th>//www.simplilearn.com</th>\n",
       "      <th>(</th>\n",
       "      <th>attn.editors</th>\n",
       "      <th>above</th>\n",
       "      <th>press</th>\n",
       "      <th>release</th>\n",
       "      <th>comes</th>\n",
       "      <th>you</th>\n",
       "      <th>under</th>\n",
       "      <th>an</th>\n",
       "      <th>arrangement</th>\n",
       "      <th>wire</th>\n",
       "      <th>india</th>\n",
       "      <th>hindu</th>\n",
       "      <th>takes</th>\n",
       "      <th>no</th>\n",
       "      <th>editorial</th>\n",
       "      <th>responsibility</th>\n",
       "      <th>same</th>\n",
       "      <th>)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simplilearn</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             simplilearn  is  ranked  as  ...  editorial  responsibility  same  )\n",
       "Simplilearn            7   5       1   3  ...          1               1     1  1\n",
       "\n",
       "[1 rows x 202 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(fdist, index=['Simplilearn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJHMRCk6LlRt"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, blankline_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wvh_zhC-MYMn",
    "outputId": "bb619025-1f60-4142-c337-d54d097adce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8', '50', '50', '8', '400', '500', '000', '2010', '150', '40']"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(Text,pattern='\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mnclv-FMcbx"
   },
   "outputs": [],
   "source": [
    "data_bl_tokenize=blankline_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "lETH04qYMqlj",
    "outputId": "57944633-71da-4abd-df84-cbb589079071"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Simplilearn is ranked as #8 of the 50 most influential education brands outpacing Wharton University, Harvard Business School, New York University, University of Phoenix and other traditional institutions. Organizations were honored for their achievements in the areas of educational publishing and their commitment to providing quality content. As one of the top among 50 organizations receiving recognition, Simplilearn was selected as #8 based on its engaging work on LinkedIn.',\n",
       " \"Michael Stebbins, Simplilearn's Chief Innovation Officer, said “This is another validation that skill-centric education is the new way to improve careers. We're among good company with LinkedIn and others who join us in engaging professional audiences with information that not only educates but also inspires them to reach their career goals.”\",\n",
       " 'Education sector marketers have flocked to LinkedIn to reach the professional networking platform’s audience of professionals, who are keen to boost their careers and earning power with further education, according to the report.',\n",
       " 'Simplilearn is the world’s largest provider of short-term certification courses that address unique learning needs of working professionals. Courses include a variety of topics in the business, technology, and design fields, spanning PMP, Big Data, Digital Marketing, Data Science, Cloud Computing and many more. With over 400 courses offered, Simplilearn has trained more than 500,000 professionals since its inception in April 2010. With its footprint in over 150 countries, Simplilearn’s patrons are assured of up-skilling and re-skilling themselves for faster and surer career growth. Simplilearn’s course content is recognized by more than 40 global accreditors, including PMI, Axelos, Scrum Alliance, OMCP, Peoplecert, and The Open Group. For more information, visit http://www.simplilearn.com \\n(Attn.editors: The above press release comes to you under an arrangement with Business Wire India. THE HINDU takes no Editorial responsibility for the same.)']"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bl_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lEnU8jcNMtr5",
    "outputId": "e79f05b1-f12d-4524-dc2e-be66c9606f4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_bl_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "WYyFn-hGM6tA",
    "outputId": "bee00739-f8f5-424f-8292-76f7b8813e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simplilearn is ranked as #8 of the 50 most influential education brands outpacing Wharton University, Harvard Business School, New York University, University of Phoenix and other traditional institutions. Organizations were honored for their achievements in the areas of educational publishing and their commitment to providing quality content. As one of the top among 50 organizations receiving recognition, Simplilearn was selected as #8 based on its engaging work on LinkedIn.'"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bl_tokenize[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujaRC8qOM_Mq"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ-FybdpNBwX"
   },
   "outputs": [],
   "source": [
    "data_sent_tokenize=sent_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "iYsmjdEJNFxv",
    "outputId": "39c04f83-14a5-4acf-e266-cd0878cfb30b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Simplilearn is ranked as #8 of the 50 most influential education brands outpacing Wharton University, Harvard Business School, New York University, University of Phoenix and other traditional institutions.',\n",
       " 'Organizations were honored for their achievements in the areas of educational publishing and their commitment to providing quality content.',\n",
       " 'As one of the top among 50 organizations receiving recognition, Simplilearn was selected as #8 based on its engaging work on LinkedIn.',\n",
       " \"Michael Stebbins, Simplilearn's Chief Innovation Officer, said “This is another validation that skill-centric education is the new way to improve careers.\",\n",
       " \"We're among good company with LinkedIn and others who join us in engaging professional audiences with information that not only educates but also inspires them to reach their career goals.”\\n\\nEducation sector marketers have flocked to LinkedIn to reach the professional networking platform’s audience of professionals, who are keen to boost their careers and earning power with further education, according to the report.\",\n",
       " 'Simplilearn is the world’s largest provider of short-term certification courses that address unique learning needs of working professionals.',\n",
       " 'Courses include a variety of topics in the business, technology, and design fields, spanning PMP, Big Data, Digital Marketing, Data Science, Cloud Computing and many more.',\n",
       " 'With over 400 courses offered, Simplilearn has trained more than 500,000 professionals since its inception in April 2010.',\n",
       " 'With its footprint in over 150 countries, Simplilearn’s patrons are assured of up-skilling and re-skilling themselves for faster and surer career growth.',\n",
       " 'Simplilearn’s course content is recognized by more than 40 global accreditors, including PMI, Axelos, Scrum Alliance, OMCP, Peoplecert, and The Open Group.',\n",
       " 'For more information, visit http://www.simplilearn.com \\n(Attn.editors: The above press release comes to you under an arrangement with Business Wire India.',\n",
       " 'THE HINDU takes no Editorial responsibility for the same.)']"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJ3czRqcNIBD"
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnKdiqvJNLfO"
   },
   "outputs": [],
   "source": [
    "string = \"welcome to Simplilearn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfdbBeXoNRXY"
   },
   "outputs": [],
   "source": [
    "\n",
    "string_tokens=nltk.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "agvAcNOuNWaB",
    "outputId": "3d49822e-8e52-4a46-be52-26a3fbc21aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome', 'to', 'Simplilearn']"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MNM0kKWNXqj"
   },
   "outputs": [],
   "source": [
    "string_bigrams=list(nltk.bigrams(string_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z-m528QkNfEW",
    "outputId": "63839ec6-2cb7-4b38-cab4-17d7ffa5fdfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('welcome', 'to'), ('to', 'Simplilearn')]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z15rD0pXNgec"
   },
   "outputs": [],
   "source": [
    "string_trigrams=list(nltk.trigrams(string_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8kmBoH9qNmRx",
    "outputId": "91f9d780-fc39-42d7-a81e-08432ef555ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('welcome', 'to', 'Simplilearn')]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUJe2qzbNnpJ"
   },
   "outputs": [],
   "source": [
    "string_ngrams=list(nltk.ngrams(string_tokens, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iWQAqm_INtnF",
    "outputId": "d479447a-b033-4a3e-996d-2c261c797db6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BqSR272NvMt"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FyrGxy0Nzho"
   },
   "outputs": [],
   "source": [
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qlLQiskzN1RO",
    "outputId": "adfa5fe3-73d0-4511-9b7c-046d6b130f15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNDavrZWN4Yi"
   },
   "outputs": [],
   "source": [
    "words_to_stem=[\"giving\", \"gave\", \"given\", \"will give\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "XRUkl3POOFCv",
    "outputId": "107c52b2-40c3-4f70-af39-e605e3468b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:give\n",
      "gave:gave\n",
      "given:given\n",
      "will give:will giv\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wm3BpRLPOMTd"
   },
   "outputs": [],
   "source": [
    "lst=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "3AQrT0YOOSwI",
    "outputId": "27177546-2890-41c7-a3fd-2deb0fdb4a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "gave:gav\n",
      "given:giv\n",
      "will give:will give\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gO7N9e5OUuz"
   },
   "outputs": [],
   "source": [
    "sbst=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "v0gGFHxLOYCz",
    "outputId": "948594b7-53f2-4c50-8db0-b4de6efaa769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbst.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ye3RCrWrOZ8R",
    "outputId": "97491b5d-4c0b-4794-a992-53580cf4aa2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbst.stem('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WLZauLtKOdmW",
    "outputId": "7829d017-669f-4a4d-f8d2-764d878c03a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:give\n",
      "gave:gave\n",
      "given:given\n",
      "will give:will giv\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +sbst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCpP8CnUOg8_"
   },
   "outputs": [],
   "source": [
    "def stemms(word):\n",
    "    print(\"Porter:\"+pst.stem(word))\n",
    "    print(\"Lancaster:\"+lst.stem(word))\n",
    "    print(\"Snowball:\"+sbst.stem(word))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kGo7ndUTOkgR",
    "outputId": "95b4d864-1fa2-4a02-8bf7-2a76ba224a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter:take\n",
      "Lancaster:tak\n",
      "Snowball:take\n"
     ]
    }
   ],
   "source": [
    "stemms('taking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iUe6xBdSOnJt",
    "outputId": "1858caf2-ad84-4b4e-9148-18fd1be85ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter:data\n",
      "Lancaster:dat\n",
      "Snowball:data\n"
     ]
    }
   ],
   "source": [
    "stemms('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "W2ybbXRUOsMW",
    "outputId": "365a18b0-f6f8-44e7-963e-2fce11823639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phvwV7AdOwBB"
   },
   "outputs": [],
   "source": [
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Rfce1fsOxmw",
    "outputId": "b06776ae-861f-4f09-da29-f90e1be82da6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QSr6sRZAOzOb",
    "outputId": "6164a589-1fe1-44d9-a0c9-c2cf8f6f6e68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'curriculum'"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('curricula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "4-YB6JsXO15N",
    "outputId": "9b6d48a7-7e81-4279-a793-bd4c2a164baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giving\n",
      "gave:gave\n",
      "given:given\n",
      "will give:will give\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ewqkVrUgO3n9",
    "outputId": "44d0f876-aae1-4da8-e6b5-253a6ce05d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giving\n",
      "gave:gave\n",
      "given:given\n",
      "will give:will give\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +word_lem.lemmatize(words,wordnet.NOUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "6AsT6cWdO5-F",
    "outputId": "a77eb90e-bb68-4461-ddd4-410dc852a288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pBFwN4heO72a",
    "outputId": "bfafcded-39a8-4e19-93bf-bfb01ee39c7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x080uznsO9PX",
    "outputId": "79e5ee9e-bfd1-406a-f1b9-459cf6d40ea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LclLfUEtPAMl"
   },
   "outputs": [],
   "source": [
    "fdist_top10=fdist.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "MTD59QZPPCBv",
    "outputId": "d588b97e-069d-4ec5-b1c2-a825d66252fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 24),\n",
       " ('the', 12),\n",
       " ('.', 12),\n",
       " ('of', 9),\n",
       " ('and', 9),\n",
       " ('to', 8),\n",
       " ('simplilearn', 7),\n",
       " ('with', 6),\n",
       " ('is', 5),\n",
       " ('in', 5)]"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xKbp2Am4PMEY",
    "outputId": "e3b65548-24f7-4138-e01f-cb8e301113d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SL_word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oK2OkuJPPWKp"
   },
   "outputs": [],
   "source": [
    "punctuation=re.compile(r'[-.?!,:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAsZaYutPcIv"
   },
   "outputs": [],
   "source": [
    "post_punctuation=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-G6f2zGOPezA"
   },
   "outputs": [],
   "source": [
    "for words in SL_word_tokenize:\n",
    "    word=punctuation.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V1XlFWg0Pgdb",
    "outputId": "02a20f66-3505-45f0-b172-ca6cc9de1ced"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_punctuation) #list after removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiI1pAHBPjFj"
   },
   "outputs": [],
   "source": [
    "post_stop_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L5j_wKw2PuNN",
    "outputId": "a6cf96c7-cecf-4e25-ff3c-e72d8b3f4113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stp_words=stopwords.words('english')\n",
    "stp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AhpiBCyPvrm"
   },
   "outputs": [],
   "source": [
    "for words in post_punctuation:\n",
    "    words=words.lower()\n",
    "    if words not in stp_words:\n",
    "        post_stop_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "INER9NDUPzaO",
    "outputId": "095c15cd-8f1a-4c3a-9111-e789918b00bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QDEcN_UP08b"
   },
   "outputs": [],
   "source": [
    "fdist2=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTouWRzvP2sI"
   },
   "outputs": [],
   "source": [
    "for word in post_stop_words:\n",
    "    fdist2[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wcWiUTshP4Mp",
    "outputId": "388b352a-0f06-48ad-9aa2-1c507794ca63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "8x8WKAUSP6ZC",
    "outputId": "14934fc4-57a8-468a-a4b9-e9be59edf545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simplilearn', 7),\n",
       " ('education', 4),\n",
       " ('’', 4),\n",
       " ('university', 3),\n",
       " ('business', 3),\n",
       " ('linkedin', 3),\n",
       " ('professionals', 3),\n",
       " ('courses', 3),\n",
       " ('#', 2),\n",
       " ('new', 2)]"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "26V2U8GOP8Aj",
    "outputId": "5c6e50bd-197f-4a6f-edd8-be05432f2e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 24),\n",
       " ('the', 12),\n",
       " ('.', 12),\n",
       " ('of', 9),\n",
       " ('and', 9),\n",
       " ('to', 8),\n",
       " ('simplilearn', 7),\n",
       " ('with', 6),\n",
       " ('is', 5),\n",
       " ('in', 5)]"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zygG4UERP_aN",
    "outputId": "94938e00-db2a-4a10-c298-d070c8f22463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('Simplilearn', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('ranked', 'NNS')]\n",
      "[('as', 'IN')]\n",
      "[('#', '#')]\n",
      "[('8', 'CD')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('50', 'CD')]\n",
      "[('most', 'JJS')]\n",
      "[('influential', 'JJ')]\n",
      "[('education', 'NN')]\n",
      "[('brands', 'NNS')]\n",
      "[('outpacing', 'VBG')]\n",
      "[('Wharton', 'NNP')]\n",
      "[('University', 'NNP')]\n",
      "[(',', ',')]\n",
      "[('Harvard', 'NNP')]\n",
      "[('Business', 'NN')]\n",
      "[('School', 'NN')]\n",
      "[(',', ',')]\n",
      "[('New', 'NNP')]\n",
      "[('York', 'NNP')]\n",
      "[('University', 'NNP')]\n",
      "[(',', ',')]\n",
      "[('University', 'NNP')]\n",
      "[('of', 'IN')]\n",
      "[('Phoenix', 'NNP')]\n",
      "[('and', 'CC')]\n",
      "[('other', 'JJ')]\n",
      "[('traditional', 'JJ')]\n",
      "[('institutions', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('Organizations', 'NNS')]\n",
      "[('were', 'VBD')]\n",
      "[('honored', 'VBN')]\n",
      "[('for', 'IN')]\n",
      "[('their', 'PRP$')]\n",
      "[('achievements', 'NNS')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('areas', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('educational', 'JJ')]\n",
      "[('publishing', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('their', 'PRP$')]\n",
      "[('commitment', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('providing', 'VBG')]\n",
      "[('quality', 'NN')]\n",
      "[('content', 'NN')]\n",
      "[('.', '.')]\n",
      "[('As', 'IN')]\n",
      "[('one', 'CD')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('top', 'NN')]\n",
      "[('among', 'IN')]\n",
      "[('50', 'CD')]\n",
      "[('organizations', 'NNS')]\n",
      "[('receiving', 'VBG')]\n",
      "[('recognition', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Simplilearn', 'NN')]\n",
      "[('was', 'VBD')]\n",
      "[('selected', 'VBN')]\n",
      "[('as', 'IN')]\n",
      "[('#', '#')]\n",
      "[('8', 'CD')]\n",
      "[('based', 'VBN')]\n",
      "[('on', 'IN')]\n",
      "[('its', 'PRP$')]\n",
      "[('engaging', 'VBG')]\n",
      "[('work', 'NN')]\n",
      "[('on', 'IN')]\n",
      "[('LinkedIn', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Michael', 'NNP')]\n",
      "[('Stebbins', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('Simplilearn', 'NN')]\n",
      "[(\"'s\", 'POS')]\n",
      "[('Chief', 'NN')]\n",
      "[('Innovation', 'NN')]\n",
      "[('Officer', 'NN')]\n",
      "[(',', ',')]\n",
      "[('said', 'VBD')]\n",
      "[('“', 'NN')]\n",
      "[('This', 'DT')]\n",
      "[('is', 'VBZ')]\n",
      "[('another', 'DT')]\n",
      "[('validation', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('skill-centric', 'JJ')]\n",
      "[('education', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('new', 'JJ')]\n",
      "[('way', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('improve', 'VB')]\n",
      "[('careers', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('We', 'PRP')]\n",
      "[(\"'re\", 'VBP')]\n",
      "[('among', 'IN')]\n",
      "[('good', 'JJ')]\n",
      "[('company', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('LinkedIn', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('others', 'NNS')]\n",
      "[('who', 'WP')]\n",
      "[('join', 'NN')]\n",
      "[('us', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('engaging', 'VBG')]\n",
      "[('professional', 'JJ')]\n",
      "[('audiences', 'NNS')]\n",
      "[('with', 'IN')]\n",
      "[('information', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('not', 'RB')]\n",
      "[('only', 'RB')]\n",
      "[('educates', 'NNS')]\n",
      "[('but', 'CC')]\n",
      "[('also', 'RB')]\n",
      "[('inspires', 'NNS')]\n",
      "[('them', 'PRP')]\n",
      "[('to', 'TO')]\n",
      "[('reach', 'NN')]\n",
      "[('their', 'PRP$')]\n",
      "[('career', 'NN')]\n",
      "[('goals.', 'NN')]\n",
      "[('”', 'NN')]\n",
      "[('Education', 'NN')]\n",
      "[('sector', 'NN')]\n",
      "[('marketers', 'NNS')]\n",
      "[('have', 'VB')]\n",
      "[('flocked', 'VBN')]\n",
      "[('to', 'TO')]\n",
      "[('LinkedIn', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('reach', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('professional', 'JJ')]\n",
      "[('networking', 'NN')]\n",
      "[('platform', 'NN')]\n",
      "[('’', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('audience', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('professionals', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('who', 'WP')]\n",
      "[('are', 'VBP')]\n",
      "[('keen', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('boost', 'NN')]\n",
      "[('their', 'PRP$')]\n",
      "[('careers', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('earning', 'VBG')]\n",
      "[('power', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('further', 'RB')]\n",
      "[('education', 'NN')]\n",
      "[(',', ',')]\n",
      "[('according', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('the', 'DT')]\n",
      "[('report', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Simplilearn', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('world', 'NN')]\n",
      "[('’', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('largest', 'JJS')]\n",
      "[('provider', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('short-term', 'JJ')]\n",
      "[('certification', 'NN')]\n",
      "[('courses', 'NNS')]\n",
      "[('that', 'IN')]\n",
      "[('address', 'NN')]\n",
      "[('unique', 'NN')]\n",
      "[('learning', 'VBG')]\n",
      "[('needs', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('working', 'VBG')]\n",
      "[('professionals', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('Courses', 'NNS')]\n",
      "[('include', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('variety', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('topics', 'NNS')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('business', 'NN')]\n",
      "[(',', ',')]\n",
      "[('technology', 'NN')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('design', 'NN')]\n",
      "[('fields', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('spanning', 'VBG')]\n",
      "[('PMP', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Big', 'JJ')]\n",
      "[('Data', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('Digital', 'NNP')]\n",
      "[('Marketing', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Data', 'NNS')]\n",
      "[('Science', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Cloud', 'NN')]\n",
      "[('Computing', 'VBG')]\n",
      "[('and', 'CC')]\n",
      "[('many', 'JJ')]\n",
      "[('more', 'RBR')]\n",
      "[('.', '.')]\n",
      "[('With', 'IN')]\n",
      "[('over', 'IN')]\n",
      "[('400', 'CD')]\n",
      "[('courses', 'NNS')]\n",
      "[('offered', 'VBN')]\n",
      "[(',', ',')]\n",
      "[('Simplilearn', 'NN')]\n",
      "[('has', 'VBZ')]\n",
      "[('trained', 'VBN')]\n",
      "[('more', 'RBR')]\n",
      "[('than', 'IN')]\n",
      "[('500,000', 'CD')]\n",
      "[('professionals', 'NNS')]\n",
      "[('since', 'IN')]\n",
      "[('its', 'PRP$')]\n",
      "[('inception', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('April', 'NNP')]\n",
      "[('2010', 'CD')]\n",
      "[('.', '.')]\n",
      "[('With', 'IN')]\n",
      "[('its', 'PRP$')]\n",
      "[('footprint', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('over', 'IN')]\n",
      "[('150', 'CD')]\n",
      "[('countries', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('Simplilearn', 'NN')]\n",
      "[('’', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('patrons', 'NNS')]\n",
      "[('are', 'VBP')]\n",
      "[('assured', 'VBN')]\n",
      "[('of', 'IN')]\n",
      "[('up-skilling', 'JJ')]\n",
      "[('and', 'CC')]\n",
      "[('re-skilling', 'NN')]\n",
      "[('themselves', 'PRP')]\n",
      "[('for', 'IN')]\n",
      "[('faster', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('surer', 'NN')]\n",
      "[('career', 'NN')]\n",
      "[('growth', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Simplilearn', 'NN')]\n",
      "[('’', 'NN')]\n",
      "[('s', 'NN')]\n",
      "[('course', 'NN')]\n",
      "[('content', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('recognized', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('more', 'RBR')]\n",
      "[('than', 'IN')]\n",
      "[('40', 'CD')]\n",
      "[('global', 'JJ')]\n",
      "[('accreditors', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('including', 'VBG')]\n",
      "[('PMI', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Axelos', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Scrum', 'NN')]\n",
      "[('Alliance', 'NN')]\n",
      "[(',', ',')]\n",
      "[('OMCP', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Peoplecert', 'NN')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('The', 'DT')]\n",
      "[('Open', 'VB')]\n",
      "[('Group', 'NNP')]\n",
      "[('.', '.')]\n",
      "[('For', 'IN')]\n",
      "[('more', 'RBR')]\n",
      "[('information', 'NN')]\n",
      "[(',', ',')]\n",
      "[('visit', 'NN')]\n",
      "[('http', 'NN')]\n",
      "[(':', ':')]\n",
      "[('//www.simplilearn.com', 'NN')]\n",
      "[('(', '(')]\n",
      "[('Attn.editors', 'NNS')]\n",
      "[(':', ':')]\n",
      "[('The', 'DT')]\n",
      "[('above', 'IN')]\n",
      "[('press', 'NN')]\n",
      "[('release', 'NN')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('you', 'PRP')]\n",
      "[('under', 'IN')]\n",
      "[('an', 'DT')]\n",
      "[('arrangement', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('Business', 'NN')]\n",
      "[('Wire', 'NN')]\n",
      "[('India', 'NNP')]\n",
      "[('.', '.')]\n",
      "[('THE', 'DT')]\n",
      "[('HINDU', 'NN')]\n",
      "[('takes', 'VBZ')]\n",
      "[('no', 'DT')]\n",
      "[('Editorial', 'NN')]\n",
      "[('responsibility', 'NN')]\n",
      "[('for', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('same', 'JJ')]\n",
      "[('.', '.')]\n",
      "[(')', ')')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Simplilearn is ranked as #8 of the 50 most influential education brands outpacing Wharton University, Harvard Business School, New York University, University of Phoenix and other traditional institutions. Organizations were honored for their achievements in the areas of educational publishing and their commitment to providing quality content. As one of the top among 50 organizations receiving recognition, Simplilearn was selected as #8 based on its engaging work on LinkedIn.\n",
    "\n",
    "Michael Stebbins, Simplilearn's Chief Innovation Officer, said “This is another validation that skill-centric education is the new way to improve careers. We're among good company with LinkedIn and others who join us in engaging professional audiences with information that not only educates but also inspires them to reach their career goals.”\n",
    "\n",
    "Education sector marketers have flocked to LinkedIn to reach the professional networking platform’s audience of professionals, who are keen to boost their careers and earning power with further education, according to the report.\n",
    "\n",
    "Simplilearn is the world’s largest provider of short-term certification courses that address unique learning needs of working professionals. Courses include a variety of topics in the business, technology, and design fields, spanning PMP, Big Data, Digital Marketing, Data Science, Cloud Computing and many more. With over 400 courses offered, Simplilearn has trained more than 500,000 professionals since its inception in April 2010. With its footprint in over 150 countries, Simplilearn’s patrons are assured of up-skilling and re-skilling themselves for faster and surer career growth. Simplilearn’s course content is recognized by more than 40 global accreditors, including PMI, Axelos, Scrum Alliance, OMCP, Peoplecert, and The Open Group. For more information, visit http://www.simplilearn.com \n",
    "(Attn.editors: The above press release comes to you under an arrangement with Business Wire India. THE HINDU takes no Editorial responsibility for the same.)\"\"\"\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "for token in sentence_tokens:\n",
    "  print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gdllo_JVyrH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLvQCxfIMNlC"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('amazonLabelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "JmVmuzLtMPOX",
    "outputId": "ff58c24e-99de-4296-cf10-00da73066c54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S                                           Feedback Sentiment\n",
       "0  1                        Good case, Excellent value.  Positive\n",
       "1  2                             Great for the jawbone.  Positive\n",
       "2  3  Tied to charger for conversations lasting more...  Negative\n",
       "3  4                                  The mic is great.  Positive\n",
       "4  5  I have to jiggle the plug to get it to line up...  Negative"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-ew0Up2MQv8"
   },
   "outputs": [],
   "source": [
    "x = data['Feedback']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ4VJDOMM9sM"
   },
   "outputs": [],
   "source": [
    "y = data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InfjfMP_MSjZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUNCfKUwMVKQ"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=True,stop_words='english',min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvBLpFxNMWp6"
   },
   "outputs": [],
   "source": [
    "#X_count_vect = (count_vect.fit_transform(x))\n",
    "x_count_vect = count_vect.fit_transform(x.values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0SyFwXH_MbRK",
    "outputId": "541ee1aa-8648-4687-e579-7828b42ee33a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 611)"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FE1EFc_rMims"
   },
   "outputs": [],
   "source": [
    "X_names= count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6UD0jR4Mmg6"
   },
   "outputs": [],
   "source": [
    "x_count_vect = pd.DataFrame(x_count_vect.toarray(), columns=X_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "qD6vAt8MMpfI",
    "outputId": "204a403b-9b90-4ec5-81d0-406785c85892"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>50</th>\n",
       "      <th>510</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>actually</th>\n",
       "      <th>additional</th>\n",
       "      <th>adorable</th>\n",
       "      <th>advertised</th>\n",
       "      <th>advise</th>\n",
       "      <th>ago</th>\n",
       "      <th>allows</th>\n",
       "      <th>amazed</th>\n",
       "      <th>amazon</th>\n",
       "      <th>answer</th>\n",
       "      <th>appears</th>\n",
       "      <th>area</th>\n",
       "      <th>arrived</th>\n",
       "      <th>ask</th>\n",
       "      <th>att</th>\n",
       "      <th>audio</th>\n",
       "      <th>auto</th>\n",
       "      <th>avoid</th>\n",
       "      <th>away</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awful</th>\n",
       "      <th>background</th>\n",
       "      <th>bad</th>\n",
       "      <th>bar</th>\n",
       "      <th>barely</th>\n",
       "      <th>bargain</th>\n",
       "      <th>bars</th>\n",
       "      <th>basically</th>\n",
       "      <th>batteries</th>\n",
       "      <th>battery</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>...</th>\n",
       "      <th>volume</th>\n",
       "      <th>w810i</th>\n",
       "      <th>waiting</th>\n",
       "      <th>wall</th>\n",
       "      <th>want</th>\n",
       "      <th>wanted</th>\n",
       "      <th>warning</th>\n",
       "      <th>warranty</th>\n",
       "      <th>wasn</th>\n",
       "      <th>waste</th>\n",
       "      <th>wasted</th>\n",
       "      <th>way</th>\n",
       "      <th>weak</th>\n",
       "      <th>wear</th>\n",
       "      <th>wearing</th>\n",
       "      <th>website</th>\n",
       "      <th>week</th>\n",
       "      <th>weeks</th>\n",
       "      <th>went</th>\n",
       "      <th>whatsoever</th>\n",
       "      <th>white</th>\n",
       "      <th>wife</th>\n",
       "      <th>wind</th>\n",
       "      <th>wired</th>\n",
       "      <th>wireless</th>\n",
       "      <th>wise</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>worthless</th>\n",
       "      <th>worthwhile</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  20  50  510  ability  able  ...  worthwhile  wouldn  wow  wrong  year  years\n",
       "0   0   0   0    0        0     0  ...           0       0    0      0     0      0\n",
       "1   0   0   0    0        0     0  ...           0       0    0      0     0      0\n",
       "2   0   0   0    0        0     0  ...           0       0    0      0     0      0\n",
       "3   0   0   0    0        0     0  ...           0       0    0      0     0      0\n",
       "4   0   0   0    0        0     0  ...           0       0    0      0     0      0\n",
       "\n",
       "[5 rows x 611 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-jHd-BeMw1J"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LP0UvaynMz-E"
   },
   "outputs": [],
   "source": [
    "x_train_cv,x_test_cv,y_train_cv,y_test_cv = train_test_split(x_count_vect,y,test_size=0.25,random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jMQiQd0M1gz"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KytbXx-NNMOH"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "VcmCXLv8NORT",
    "outputId": "f05e8747-cf54-4787-a69a-3c3e0167feae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_cv, y_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFIjEj9mNV8A"
   },
   "outputs": [],
   "source": [
    "yp = model.predict(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVavoCrKNn0M"
   },
   "outputs": [],
   "source": [
    "Sentiment_Prob = model.predict_proba(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "6SER82GsNzO4",
    "outputId": "a22d6a8c-bde0-4e6f-c9a8-a583fe97fa33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0.471211</td>\n",
       "      <td>0.528789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.523771</td>\n",
       "      <td>0.476229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.594245</td>\n",
       "      <td>0.405755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.377961</td>\n",
       "      <td>0.622039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.970515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0.258911</td>\n",
       "      <td>0.741089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.451197</td>\n",
       "      <td>0.548803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.253589</td>\n",
       "      <td>0.746411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.007910</td>\n",
       "      <td>0.992090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.614739</td>\n",
       "      <td>0.385261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Negative  Positive\n",
       "544  0.471211  0.528789\n",
       "515  0.523771  0.476229\n",
       "193  0.594245  0.405755\n",
       "11   0.377961  0.622039\n",
       "279  0.029485  0.970515\n",
       "..        ...       ...\n",
       "735  0.258911  0.741089\n",
       "416  0.451197  0.548803\n",
       "601  0.253589  0.746411\n",
       "295  0.007910  0.992090\n",
       "467  0.614739  0.385261\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Sentiment_Prob, index = x_test_cv.index, columns = ['Negative', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvHlOSZpN-kf"
   },
   "outputs": [],
   "source": [
    "Accuracy = model.score(x_test_cv, y_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hhSGwOa8OZrs",
    "outputId": "ae493fd3-3a25-4d57-a14b-02b5e5294528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 145,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anBZxLMZOeol"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "18-6-2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
